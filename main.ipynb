{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64d63b4",
   "metadata": {
    "id": "e64d63b4"
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2fad78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8198,
     "status": "ok",
     "timestamp": 1759589550496,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "0c2fad78",
    "outputId": "75e7a22b-8884-4deb-b7ab-d35252c655d8"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mImportError: libsqlite3.so: cannot open shared object file: No such file or directory. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Download the Gutenberg corpus from NLTK\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Step 3: Import the Gutenberg corpus\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Step 4: Load the 'Hamlet' text\n",
    "data = gutenberg.raw('shakespeare-hamlet.txt')  # This loads the raw text of Hamlet\n",
    "\n",
    "# Step 5: Save the loaded text into a local file\n",
    "with open('hamlet.txt', 'w') as file:\n",
    "    file.write(data)  # Write the full text into 'hamlet.txt' file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12aa77",
   "metadata": {
    "id": "3c12aa77"
   },
   "source": [
    "Now that you have trained and saved your model and tokenizer, you can download them to your local machine for deployment. The following code will create a zip file containing the necessary files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a978d",
   "metadata": {
    "id": "447a978d"
   },
   "source": [
    "You can now download the `next_word_prediction_model.zip` file from the Colab file explorer (the folder icon on the left sidebar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ceaf56",
   "metadata": {
    "id": "42ceaf56"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64004481",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5735,
     "status": "ok",
     "timestamp": 1759589556243,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "64004481",
    "outputId": "1cd3de5f-86ca-4775-873b-b9b1e1dfac04"
   },
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "with open('hamlet.txt', 'r') as file:\n",
    "    text = file.read().lower()  # Read the text file and convert all text to lowercase\n",
    "\n",
    "# Step 3: Initialize the Tokenizer\n",
    "tokenizer = Tokenizer()  # This will convert words into unique integer indices\n",
    "\n",
    "# Step 4: Fit the tokenizer on the text\n",
    "tokenizer.fit_on_texts([text])  # Learn the vocabulary from the text\n",
    "\n",
    "# Step 5: Get total number of unique words\n",
    "total_words = len(tokenizer.word_index) + 1  # +1 because index starts from 1\n",
    "print(f'Total unique words: {total_words}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f96e2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1759589556402,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "e9f96e2b",
    "outputId": "446fab0a-1315-43e7-f8a4-6acab7f1c8bb"
   },
   "outputs": [],
   "source": [
    "total_words\n",
    "# how tokenize in entair data set\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61a891",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73,
     "status": "ok",
     "timestamp": 1759589556477,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "9a61a891",
    "outputId": "bd861c48-99d9-48cf-966c-c57b2aa4e59c"
   },
   "outputs": [],
   "source": [
    "# Step 1: Initialize a list to store input sequences\n",
    "input_sequences = []  # This list will hold sequences of words for training\n",
    "\n",
    "# Step 2: Loop through each line of the text\n",
    "for line in text.split('\\n'):  # Split text by newline to process line by line\n",
    "    # Step 3: Convert the line into a sequence of word indices\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]  # texts_to_sequences returns a list, take the first element\n",
    "\n",
    "    # Step 4: Create n-gram sequences from the token list\n",
    "    for i in range(1, len(token_list)):\n",
    "        # Take tokens from start to i+1 to form an n-gram\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "\n",
    "        # Step 5: Append the sequence to the input_sequences list\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Step 6: Check how many sequences were created\n",
    "print(f'Total input sequences: {len(input_sequences)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3b93c",
   "metadata": {
    "id": "b2f3b93c"
   },
   "source": [
    "\n",
    "Input Sequences: Teaches the model the relationship between a context and the next word.\n",
    "\n",
    "Sentence: \"to be or not to be\"\n",
    "Tokens: [5, 12, 3, 7, 5, 12]\n",
    "Generated Sequences:\n",
    "[5, 12] -> Target: 3\n",
    "[5, 12, 3] -> Target: 7\n",
    "[5, 12, 3, 7] -> Target: 5\n",
    "[5, 12, 3, 7, 5] -> Target: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1499a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1759589556611,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "5ed1499a",
    "outputId": "1e0d8967-20fc-4a91-a87c-6e2f697f0d0f"
   },
   "outputs": [],
   "source": [
    "# Step 1: Find the maximum sequence length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "# This is needed because LSTM models expect all input sequences to have the same length\n",
    "print(f\"Maximum sequence length: {max_sequence_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55805f0",
   "metadata": {
    "id": "a55805f0"
   },
   "source": [
    "### **Why padding is  needed?**\n",
    "\n",
    "**LSTM Requires Fixed-Length Input:**\n",
    "- LSTM or RNN models require input sequences to be of the same length.\n",
    "- However, the n-gram sequences we created have different lengths.\n",
    "\n",
    "**Padding Solves the Problem:**\n",
    "- We add zeros at the beginning of shorter sequences to make their length equal to `max_sequence_len`.\n",
    "\n",
    "**Example:**\n",
    "- Original sequence: `[5, 12, 3]`\n",
    "- Max length = 6\n",
    "- After padding: `[0, 0, 0, 5, 12, 3]`\n",
    "\n",
    "**The Model Understands:**\n",
    "- The model learns to ignore the zero padding and focuses on learning to predict the next word from the proper context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70536b22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1759589556641,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "70536b22",
    "outputId": "0633af79-04a7-4a6e-ce11-a4619a5cba72"
   },
   "outputs": [],
   "source": [
    "# Step 1: Import pad_sequences (already imported earlier)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Step 2: Pad all input sequences to have the same length\n",
    "input_sequences = np.array(pad_sequences(input_sequences,\n",
    "                                        maxlen=max_sequence_len,  # Pad all sequences to max length\n",
    "                                        padding='pre'))           # Add padding at the beginning\n",
    "\n",
    "# Step 3: Check the padded sequences\n",
    "print(input_sequences[:5])  # Display first 5 sequences to see padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8488f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1759589556852,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "b5a8488f",
    "outputId": "26cd779f-fd31-4856-8cb4-5c9611066030"
   },
   "outputs": [],
   "source": [
    "# Step 1: Import TensorFlow (already imported)\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 2: Split input_sequences into predictors (X) and label (y)\n",
    "x = input_sequences[:, :-1]  # All words except the last one are inputs (predictors)\n",
    "y = input_sequences[:, -1]   # The last word in each sequence is the target (label)\n",
    "\n",
    "# Step 3: Check shapes\n",
    "print(f'X shape: {x.shape}')\n",
    "print(f'y shape: {y.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06297dc9",
   "metadata": {
    "id": "06297dc9"
   },
   "source": [
    "### **Why is this step needed?**\n",
    "\n",
    "**Predictors (X):**\n",
    "- The model learns to predict the next word from these sequences.\n",
    "- Example: `[0, 0, 0, 5, 12]` → The model will predict the next word based on this context.\n",
    "\n",
    "**Label (y):**\n",
    "- This is the target word that the model needs to predict.\n",
    "- Example: From the sequence `[0, 0, 0, 5, 12, 3]`, the number `3` will be the target.\n",
    "\n",
    "**Training-ready format:**\n",
    "- Now we can one-hot encode `y` because the LSTM output will be categorical (with vocabulary size number of classes).\n",
    "- This creates the final format needed for training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e26dea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1759589556859,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "27e26dea",
    "outputId": "bd877e29-e065-46fc-82f2-85ca832c8cf8"
   },
   "outputs": [],
   "source": [
    "# Step 1: One-hot encode the target labels\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "# Converts each integer label into a vector of length = total_words\n",
    "# Example: if total_words = 100, label 5 → [0,0,0,0,0,1,0,...,0]\n",
    "\n",
    "# Step 2: Check the shape of y\n",
    "print(f'One-hot encoded y shape: {y.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8aca9f",
   "metadata": {
    "id": "2c8aca9f"
   },
   "source": [
    "# Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9f2af",
   "metadata": {
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1759589557248,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "d1a9f2af"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lp2CqdWSd2l_",
   "metadata": {
    "id": "Lp2CqdWSd2l_"
   },
   "source": [
    "Model Trinig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9d810",
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1759589557272,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "a3a9d810"
   },
   "outputs": [],
   "source": [
    "# Step 1: Import EarlyStopping callback from Keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Step 2: Define EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # Monitor the validation loss during training\n",
    "    patience=3,                # Stop training if val_loss doesn't improve for 3 consecutive epochs\n",
    "    restore_best_weights=True  # After stopping, restore model weights from the epoch with the best val_loss\n",
    ")\n",
    "\n",
    "# Step 3: Why we use it?\n",
    "# - Prevents overfitting by stopping training early\n",
    "# - Saves time by not training unnecessary epochs\n",
    "# - Ensures the model keeps the best weights observed during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c2a31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1759589557537,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "389c2a31",
    "outputId": "a09bbe7f-fb4c-4165-f974-66131b509073"
   },
   "outputs": [],
   "source": [
    "# Step 1: Import necessary layers and model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Step 2: Define the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Step 3: Add Embedding layer\n",
    "model.add(Embedding(\n",
    "    input_dim=total_words,     # Vocabulary size\n",
    "    output_dim=100,            # Embedding vector size\n",
    "    input_length=max_sequence_len-1  # Input sequence length (excluding target word)\n",
    "))\n",
    "# Embedding layer converts word indices into dense vectors\n",
    "\n",
    "# Step 4: Add first LSTM layer\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "# return_sequences=True because we will stack another LSTM on top\n",
    "\n",
    "# Step 5: Add Dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Step 6: Add second LSTM layer\n",
    "model.add(LSTM(100))\n",
    "# return_sequences=False by default, outputs the last hidden state\n",
    "\n",
    "# Step 7: Add Dense output layer with softmax activation\n",
    "model.add(Dense(total_words, activation=\"softmax\"))\n",
    "# Predicts probability for each word in the vocabulary\n",
    "\n",
    "# Step 8: Compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",  # Suitable for multi-class classification\n",
    "    optimizer='adam',                 # Adam optimizer for faster convergence\n",
    "    metrics=['accuracy']              # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Explicitly build the model with the input shape\n",
    "model.build(input_shape=(None, max_sequence_len - 1))\n",
    "\n",
    "\n",
    "# Step 9: Show model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb28d6b",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1759589557557,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "4bb28d6b"
   },
   "outputs": [],
   "source": [
    "# ## GRU RNN\n",
    "# ## Define the model\n",
    "# model=Sequential()\n",
    "# model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "# model.add(GRU(150,return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(GRU(100))\n",
    "# model.add(Dense(total_words,activation=\"softmax\"))\n",
    "\n",
    "# # #Compile the model\n",
    "# model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "# model.build(input_shape=(None, max_sequence_len-1))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9192c4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1863248,
     "status": "ok",
     "timestamp": 1759591420829,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "d9192c4f",
    "outputId": "5c8b3be5-979d-4ea6-b7a6-faa3dcb07ee1"
   },
   "outputs": [],
   "source": [
    "history=model.fit(x_train,y_train,epochs=50,validation_data=(x_test,y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74504b",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1759591420833,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "8c74504b"
   },
   "outputs": [],
   "source": [
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5118ca7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1759591421442,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "d5118ca7",
    "outputId": "df370e9e-a2ae-4ab2-8cdf-c801593eac46"
   },
   "outputs": [],
   "source": [
    "input_text=\"I love AI \"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word PRediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5123f4b",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1759591421445,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "a5123f4b"
   },
   "outputs": [],
   "source": [
    "## Save the model\n",
    "model.save(\"next_word_lstm.keras\")\n",
    "## Save the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63764883",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1759591421648,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "63764883",
    "outputId": "a06b53f0-2029-4c80-cc7a-393b61579d25"
   },
   "outputs": [],
   "source": [
    "input_text=\"  Barn. Last night of all,When yond same\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word PRediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FTevBJ24uBTq",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1759591421660,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "FTevBJ24uBTq"
   },
   "outputs": [],
   "source": [
    "# Folder Create and download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a956a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1759591421683,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "d34a956a",
    "outputId": "fc9648eb-e717-4829-ce63-229ee74acc4a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0ece7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1759591421694,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "75b0ece7",
    "outputId": "12053140-9bf8-46af-ebea-f2234cd21701"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# List of files to include\n",
    "files_to_zip = ['next_word_lstm.keras', 'tokenizer.pickle']\n",
    "zip_filename = 'next_word_prediction_model.zip'\n",
    "\n",
    "# Create a zip archive\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in files_to_zip:\n",
    "        zipf.write(file)  # Add file to zip\n",
    "\n",
    "print(f\"Created {zip_filename} containing the model and tokenizer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mYW9edPFwrmE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1759591421700,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "mYW9edPFwrmE",
    "outputId": "f0ccd529-18d8-4cab-d4ab-6a2efa1acd34"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(zip_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8WH-_gUbwroI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1759591421709,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "8WH-_gUbwroI",
    "outputId": "b1c92bc8-bf7b-44e2-fa70-d9b06b9017ca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wZO2CHf5wrqo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11239,
     "status": "ok",
     "timestamp": 1759591432950,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "wZO2CHf5wrqo",
    "outputId": "1a3b2ad6-1c8a-446c-aa1d-e9754941733c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TY1ULPKAHFBf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 923,
     "status": "ok",
     "timestamp": 1759591433875,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "TY1ULPKAHFBf",
    "outputId": "4b3b1048-3a89-4c05-d5c7-7418091b272c"
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "# Define a list of the packages used in the notebook\n",
    "packages = ['nltk', 'pandas', 'numpy', 'tensorflow', 'sklearn', 'streamlit', 'pickle']\n",
    "\n",
    "# Print the version of each package if it's installed\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"{package}: {pkg_resources.get_distribution(package).version}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"{package}: Not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f4424",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18351,
     "status": "ok",
     "timestamp": 1759591452228,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "ae7f4424",
    "outputId": "c442f803-bd29-4bf4-aea8-d71cfbdaee7c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1084fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1759591452240,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "ac1084fd",
    "outputId": "63e9cd0f-32d4-40fe-fd2f-bf9ea051d988"
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "# Define a list of the packages used in the notebook\n",
    "packages = ['nltk', 'pandas', 'numpy', 'tensorflow', 'sklearn', 'streamlit', 'pickle']\n",
    "\n",
    "# Print the version of each package if it's installed\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"{package}: {pkg_resources.get_distribution(package).version}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"{package}: Not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5DTXSZL2L5AB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1759591452250,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "5DTXSZL2L5AB",
    "outputId": "7b061300-d604-4d52-c794-a89a32248909"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ele5MIRLL5CD",
   "metadata": {
    "executionInfo": {
     "elapsed": 3255,
     "status": "ok",
     "timestamp": 1759591455507,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "ele5MIRLL5CD"
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oXfiAg1aL5Ey",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1759591455525,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "oXfiAg1aL5Ey"
   },
   "outputs": [],
   "source": [
    "# tokenizer JSON format save\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(\"tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(tokenizer_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78HUKNKMq4Hf",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1759591455537,
     "user": {
      "displayName": "Asux Ai",
      "userId": "06559001413637975182"
     },
     "user_tz": -360
    },
    "id": "78HUKNKMq4Hf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "skill_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
